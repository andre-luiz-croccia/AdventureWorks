{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca638c1-65f7-4d88-b9e2-c0b0a2c7b6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando extração para sales_order_detail\n",
      "Iniciando extração para sales_order_header\n",
      "Offset 0: +1000 registros (Total: 1000)\n",
      "Offset 0: +1000 registros (Total: 1000)\n",
      "Offset 1000: +1000 registros (Total: 2000)\n",
      "Offset 1000: +1000 registros (Total: 2000)\n",
      "Offset 2000: +1000 registros (Total: 3000)\n",
      "Offset 3000: +1000 registros (Total: 4000)\n",
      "Offset 4000: +1000 registros (Total: 5000)\n",
      "Offset 5000: +1000 registros (Total: 6000)\n",
      "Offset 6000: +1000 registros (Total: 7000)\n",
      "Offset 7000: +1000 registros (Total: 8000)\n",
      "Offset 8000: +1000 registros (Total: 9000)\n",
      "Offset 9000: +1000 registros (Total: 10000)\n",
      "Offset 10000: +1000 registros (Total: 11000)\n",
      "Offset 11000: +1000 registros (Total: 12000)\n",
      "Offset 12000: +1000 registros (Total: 13000)\n",
      "Offset 13000: +1000 registros (Total: 14000)\n",
      "Offset 14000: +1000 registros (Total: 15000)\n",
      "Offset 15000: +1000 registros (Total: 16000)\n",
      "Offset 16000: +1000 registros (Total: 17000)\n",
      "Offset 17000: +1000 registros (Total: 18000)\n",
      "Offset 18000: +1000 registros (Total: 19000)\n",
      "Offset 19000: +1000 registros (Total: 20000)\n",
      "Offset 20000: +1000 registros (Total: 21000)\n",
      "Offset 21000: +1000 registros (Total: 22000)\n",
      "Offset 22000: +1000 registros (Total: 23000)\n",
      "Offset 23000: +1000 registros (Total: 24000)\n",
      "Offset 24000: +1000 registros (Total: 25000)\n",
      "Offset 25000: +1000 registros (Total: 26000)\n",
      "Offset 26000: +1000 registros (Total: 27000)\n",
      "Offset 27000: +1000 registros (Total: 28000)\n",
      "Offset 28000: +1000 registros (Total: 29000)\n",
      "Offset 29000: +1000 registros (Total: 30000)\n",
      "Offset 30000: +1000 registros (Total: 31000)\n",
      "Offset 31000: +465 registros (Total: 31465)\n",
      "Concluído: ted_dev.dev_andre_silva.raw_api_sales_order_header com 31465 registros\n",
      "Schema final:\n",
      "root\n",
      " |-- SalesOrderID: long (nullable = true)\n",
      " |-- RevisionNumber: long (nullable = true)\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- DueDate: string (nullable = true)\n",
      " |-- Status: long (nullable = true)\n",
      " |-- OnlineOrderFlag: boolean (nullable = true)\n",
      " |-- SalesOrderNumber: string (nullable = true)\n",
      " |-- CustomerID: long (nullable = true)\n",
      " |-- BillToAddressID: long (nullable = true)\n",
      " |-- ShipToAddressID: long (nullable = true)\n",
      " |-- ShipMethodID: long (nullable = true)\n",
      " |-- SubTotal: double (nullable = true)\n",
      " |-- TaxAmt: double (nullable = true)\n",
      " |-- Freight: double (nullable = true)\n",
      " |-- TotalDue: double (nullable = true)\n",
      " |-- rowguid: string (nullable = true)\n",
      " |-- ModifiedDate: string (nullable = true)\n",
      " |-- ShipDate: string (nullable = true)\n",
      " |-- PurchaseOrderNumber: string (nullable = true)\n",
      " |-- AccountNumber: string (nullable = true)\n",
      " |-- SalesPersonID: double (nullable = true)\n",
      " |-- TerritoryID: long (nullable = true)\n",
      " |-- CreditCardID: double (nullable = true)\n",
      " |-- CreditCardApprovalCode: string (nullable = true)\n",
      " |-- CurrencyRateID: double (nullable = true)\n",
      " |-- Comment: void (nullable = true)\n",
      "\n",
      "Iniciando extração para purchase_order_detail\n",
      "Offset 0: +1000 registros (Total: 1000)\n",
      "Offset 1000: +1000 registros (Total: 2000)\n",
      "Offset 2000: +1000 registros (Total: 3000)\n",
      "Offset 3000: +1000 registros (Total: 4000)\n",
      "Offset 4000: +1000 registros (Total: 5000)\n",
      "Offset 5000: +1000 registros (Total: 6000)\n",
      "Offset 6000: +1000 registros (Total: 7000)\n",
      "Offset 7000: +1000 registros (Total: 8000)\n",
      "Offset 8000: +845 registros (Total: 8845)\n",
      "Concluído: ted_dev.dev_andre_silva.raw_api_purchase_order_detail com 8845 registros\n",
      "Schema final:\n",
      "root\n",
      " |-- PurchaseOrderID: long (nullable = true)\n",
      " |-- PurchaseOrderDetailID: long (nullable = true)\n",
      " |-- DueDate: string (nullable = true)\n",
      " |-- OrderQty: long (nullable = true)\n",
      " |-- ProductID: long (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- LineTotal: double (nullable = true)\n",
      " |-- ReceivedQty: double (nullable = true)\n",
      " |-- RejectedQty: double (nullable = true)\n",
      " |-- StockedQty: double (nullable = true)\n",
      " |-- ModifiedDate: string (nullable = true)\n",
      "\n",
      "Iniciando extração para purchase_order_header\n",
      "Offset 0: +1000 registros (Total: 1000)\n",
      "Offset 1000: +1000 registros (Total: 2000)\n",
      "Offset 2000: +1000 registros (Total: 3000)\n",
      "Offset 3000: +1000 registros (Total: 4000)\n",
      "Offset 4000: +12 registros (Total: 4012)\n",
      "Concluído: ted_dev.dev_andre_silva.raw_api_purchase_order_header com 4012 registros\n",
      "Schema final:\n",
      "root\n",
      " |-- PurchaseOrderID: long (nullable = true)\n",
      " |-- RevisionNumber: long (nullable = true)\n",
      " |-- Status: long (nullable = true)\n",
      " |-- EmployeeID: long (nullable = true)\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- ShipMethodID: long (nullable = true)\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- SubTotal: double (nullable = true)\n",
      " |-- TaxAmt: double (nullable = true)\n",
      " |-- Freight: double (nullable = true)\n",
      " |-- TotalDue: double (nullable = true)\n",
      " |-- ModifiedDate: string (nullable = true)\n",
      " |-- ShipDate: string (nullable = true)\n",
      "\n",
      "Erro no offset 2000: HTTPConnectionPool(host='18.209.218.63', port=8080): Read timed out. (read timeout=120)\n",
      "Concluído: ted_dev.dev_andre_silva.raw_api_sales_order_detail com 2000 registros\n",
      "Schema final:\n",
      "root\n",
      " |-- SalesOrderID: long (nullable = true)\n",
      " |-- SalesOrderDetailID: long (nullable = true)\n",
      " |-- OrderQty: long (nullable = true)\n",
      " |-- ProductID: long (nullable = true)\n",
      " |-- SpecialOfferID: long (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- UnitPriceDiscount: double (nullable = true)\n",
      " |-- LineTotal: double (nullable = true)\n",
      " |-- rowguid: string (nullable = true)\n",
      " |-- ModifiedDate: string (nullable = true)\n",
      " |-- CarrierTrackingNumber: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. EXTRAÇÃO DOS API'S SALES_ORDER_HEADER, PURCHASE_ORDER_DETAIL E PURCHASE_ORDER_HEADER\n",
    "# ========================================X=======================================\n",
    "\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "import pandas as pd\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Configuração otimizada do Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LargeAPIExtraction\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "def fetch_paginated_data(endpoint, auth):\n",
    "    base_url = \"http://18.209.218.63:8080\"\n",
    "    url = f\"{base_url}{endpoint}\"\n",
    "    limit = 1000  # Máximo de registros por página (ajuste conforme a API permitir)\n",
    "    offset = 0\n",
    "    all_data = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                auth=auth,\n",
    "                params={\"offset\": offset, \"limit\": limit},  # Parâmetros chave!\n",
    "                timeout=120\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if not data.get(\"data\"):\n",
    "                print(\"Fim dos dados alcançado.\")\n",
    "                break\n",
    "                \n",
    "            records = data[\"data\"]\n",
    "            all_data.extend(records)\n",
    "            \n",
    "            print(f\"Offset {offset}: +{len(records)} registros (Total: {len(all_data)})\")\n",
    "            \n",
    "            # Critério de parada: se a API retornar menos registros que o limite\n",
    "            if len(records) < limit:\n",
    "                break\n",
    "                \n",
    "            offset += limit  # Avança para o próximo lote\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro no offset {offset}: {str(e)}\")\n",
    "            break\n",
    "            \n",
    "    return all_data\n",
    "\n",
    "# 2. Processamento com schema evolution\n",
    "def process_endpoint(table_suffix, endpoint, auth, schema):\n",
    "    try:\n",
    "        print(f\"Iniciando extração para {table_suffix}\")\n",
    "        \n",
    "        data = fetch_paginated_data(endpoint, auth)\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"Nenhum dado retornado para {table_suffix}\")\n",
    "            return\n",
    "            \n",
    "        # Converter para DataFrame Spark\n",
    "        pdf = pd.DataFrame(data)\n",
    "        sdf = spark.createDataFrame(pdf)\n",
    "        \n",
    "        tabela_destino = f\"{schema}.raw_api_{table_suffix}\"\n",
    "        \n",
    "        # Configuração para tratamento de schema evolution\n",
    "        (sdf.write\n",
    "           .mode(\"overwrite\")\n",
    "           .format(\"delta\")\n",
    "           .option(\"mergeSchema\", \"true\")  # Permite evolução do schema\n",
    "           .option(\"overwriteSchema\", \"true\")  # Sobrescreve o schema se necessário\n",
    "           .option(\"optimizeWrite\", \"true\")\n",
    "           .saveAsTable(tabela_destino))\n",
    "        \n",
    "        # Verificação pós-escrita\n",
    "        df_check = spark.read.table(tabela_destino)\n",
    "        print(f\"Concluído: {tabela_destino} com {df_check.count()} registros\")\n",
    "        print(f\"Schema final:\")\n",
    "        df_check.printSchema()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Falha crítica no processamento de {table_suffix}: {str(e)}\")\n",
    "        raise  # Opcional: remover se quiser continuar após erros\n",
    "\n",
    "# Configurações principais\n",
    "auth = HTTPBasicAuth(\n",
    "    dbutils.secrets.get(scope=\"sqlserver_scope\", key=\"api_user\"),\n",
    "    dbutils.secrets.get(scope=\"sqlserver_scope\", key=\"api_pass\")\n",
    ")\n",
    "\n",
    "endpoints = {\n",
    "    \"sales_order_detail\": \"/SalesOrderDetail\",\n",
    "    \"sales_order_header\": \"/SalesOrderHeader\",\n",
    "    \"purchase_order_detail\": \"/PurchaseOrderDetail\",\n",
    "    \"purchase_order_header\": \"/PurchaseOrderHeader\"\n",
    "}\n",
    "\n",
    "schema = \"ted_dev.dev_andre_silva\"\n",
    "\n",
    "# 3. Execução com paralelismo controlado\n",
    "max_workers = 2  # Número de endpoints processados simultaneamente\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {\n",
    "        executor.submit(\n",
    "            process_endpoint,\n",
    "            table_suffix,\n",
    "            endpoint,\n",
    "            auth,\n",
    "            schema\n",
    "        ): table_suffix for table_suffix, endpoint in endpoints.items()\n",
    "    }\n",
    "    \n",
    "    for future in as_completed(futures):\n",
    "        table_suffix = futures[future]\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"Erro não tratado em {table_suffix}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96c3f8f-4545-442b-aa07-54b9e9750b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Iniciando extração para sales_order_detail...\n",
      "\n",
      " Offset 0 | Limit 150000 | Tentativa 1\n",
      " Erro no offset 0, tentativa 1: HTTPConnectionPool(host='18.209.218.63', port=8080): Read timed out. (read timeout=120)\n",
      " Offset 0 | Limit 150000 | Tentativa 2\n",
      " Erro no offset 0, tentativa 2: HTTPConnectionPool(host='18.209.218.63', port=8080): Read timed out. (read timeout=120)\n",
      " Offset 0 | Limit 150000 | Tentativa 3\n",
      " Offset 0: +121317 registros (Total acumulado: 121317)\n",
      "✅ Concluído: ted_dev.dev_andre_silva.raw_api_sales_order_detail com 121317 registros\n",
      "root\n",
      " |-- SalesOrderID: long (nullable = true)\n",
      " |-- SalesOrderDetailID: long (nullable = true)\n",
      " |-- OrderQty: long (nullable = true)\n",
      " |-- ProductID: long (nullable = true)\n",
      " |-- SpecialOfferID: long (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- UnitPriceDiscount: double (nullable = true)\n",
      " |-- LineTotal: double (nullable = true)\n",
      " |-- rowguid: string (nullable = true)\n",
      " |-- ModifiedDate: string (nullable = true)\n",
      " |-- CarrierTrackingNumber: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. EXTRAÇÃO APENAS DO SALES_ORDER_DETAIL\n",
    "# ==========================X=======================\n",
    "\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Spark otimizado\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LargeAPIExtraction\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "# Função de extração recursiva com divisão em blocos menores em caso de falha\n",
    "def fetch_data_recursive(endpoint, auth, offset, limit, max_retries=3, min_limit=100):\n",
    "    base_url = \"http://18.209.218.63:8080\"\n",
    "    url = f\"{base_url}{endpoint}\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\" Offset {offset} | Limit {limit} | Tentativa {attempt + 1}\")\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                auth=auth,\n",
    "                params={\"offset\": offset, \"limit\": limit},\n",
    "                timeout=120\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if not data.get(\"data\"):\n",
    "                return []\n",
    "            return data[\"data\"]\n",
    "        except Exception as e:\n",
    "            print(f\" Erro no offset {offset}, tentativa {attempt + 1}: {str(e)}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "    # Se falhar, tenta dividir o bloco\n",
    "    if limit > min_limit:\n",
    "        print(f\" Dividindo bloco de offset {offset} | Limit {limit}\")\n",
    "        mid = limit // 2\n",
    "        first_half = fetch_data_recursive(endpoint, auth, offset, mid, max_retries, min_limit)\n",
    "        second_half = fetch_data_recursive(endpoint, auth, offset + mid, limit - mid, max_retries, min_limit)\n",
    "        return first_half + second_half\n",
    "    else:\n",
    "        print(f\" Falha crítica em offset {offset} mesmo com limite mínimo ({min_limit}).\")\n",
    "        return []\n",
    "\n",
    "# Função principal de paginação\n",
    "def fetch_paginated_data(endpoint, auth, initial_limit=150000):\n",
    "    offset = 0\n",
    "    all_data = []\n",
    "\n",
    "    while True:\n",
    "        records = fetch_data_recursive(endpoint, auth, offset, initial_limit)\n",
    "        if not records:\n",
    "            break\n",
    "        all_data.extend(records)\n",
    "        print(f\" Offset {offset}: +{len(records)} registros (Total acumulado: {len(all_data)})\")\n",
    "        if len(records) < initial_limit:\n",
    "            break\n",
    "        offset += initial_limit\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Processamento e gravação Delta\n",
    "def process_endpoint(table_suffix, endpoint, auth, schema):\n",
    "    try:\n",
    "        print(f\"\\n🔄 Iniciando extração para {table_suffix}...\\n\")\n",
    "        \n",
    "        data = fetch_paginated_data(endpoint, auth)\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"⚠️ Nenhum dado retornado para {table_suffix}\")\n",
    "            return\n",
    "            \n",
    "        pdf = pd.DataFrame(data)\n",
    "        sdf = spark.createDataFrame(pdf)\n",
    "        \n",
    "        tabela_destino = f\"{schema}.raw_api_{table_suffix}\"\n",
    "        \n",
    "        sdf.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .option(\"optimizeWrite\", \"true\") \\\n",
    "            .saveAsTable(tabela_destino)\n",
    "        \n",
    "        df_check = spark.read.table(tabela_destino)\n",
    "        print(f\"✅ Concluído: {tabela_destino} com {df_check.count()} registros\")\n",
    "        df_check.printSchema()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Falha crítica no processamento de {table_suffix}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Configuração de autenticação\n",
    "auth = HTTPBasicAuth(\n",
    "    dbutils.secrets.get(scope=\"sqlserver_scope\", key=\"api_user\"),\n",
    "    dbutils.secrets.get(scope=\"sqlserver_scope\", key=\"api_pass\")\n",
    ")\n",
    "\n",
    "# Endpoint específico\n",
    "endpoints = {\n",
    "    \"sales_order_detail\": \"/SalesOrderDetail\"\n",
    "}\n",
    "\n",
    "schema = \"ted_dev.dev_andre_silva\"\n",
    "\n",
    "# Execução com ThreadPool (mesmo que tenha 1 endpoint, deixa pronto pra escalar)\n",
    "max_workers = 2\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {\n",
    "        executor.submit(\n",
    "            process_endpoint,\n",
    "            table_suffix,\n",
    "            endpoint,\n",
    "            auth,\n",
    "            schema\n",
    "        ): table_suffix for table_suffix, endpoint in endpoints.items()\n",
    "    }\n",
    "    \n",
    "    for future in as_completed(futures):\n",
    "        table_suffix = futures[future]\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"❗ Erro não tratado em {table_suffix}: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24d31488-346a-428e-a09c-bc60238e38ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>SalesOrderID</th><th>SalesOrderDetailID</th><th>OrderQty</th><th>ProductID</th><th>SpecialOfferID</th><th>UnitPrice</th><th>UnitPriceDiscount</th><th>LineTotal</th><th>rowguid</th><th>ModifiedDate</th><th>CarrierTrackingNumber</th></tr></thead><tbody><tr><td>43659</td><td>1</td><td>1</td><td>776</td><td>1</td><td>2024.994</td><td>0.0</td><td>2024.994</td><td>b207c96d-d9e6-402b-8470-2cc176c42283</td><td>2011-05-31T00:00:00</td><td>4911-403C-98</td></tr><tr><td>43659</td><td>2</td><td>3</td><td>777</td><td>1</td><td>2024.994</td><td>0.0</td><td>6074.982</td><td>7abb600d-1e77-41be-9fe5-b9142cfc08fa</td><td>2011-05-31T00:00:00</td><td>4911-403C-98</td></tr><tr><td>43659</td><td>3</td><td>1</td><td>778</td><td>1</td><td>2024.994</td><td>0.0</td><td>2024.994</td><td>475cf8c6-49f6-486e-b0ad-afc6a50cdd2f</td><td>2011-05-31T00:00:00</td><td>4911-403C-98</td></tr><tr><td>43659</td><td>4</td><td>1</td><td>771</td><td>1</td><td>2039.994</td><td>0.0</td><td>2039.994</td><td>04c4de91-5815-45d6-8670-f462719fbce3</td><td>2011-05-31T00:00:00</td><td>4911-403C-98</td></tr><tr><td>43659</td><td>5</td><td>1</td><td>772</td><td>1</td><td>2039.994</td><td>0.0</td><td>2039.994</td><td>5a74c7d2-e641-438e-a7ac-37bf23280301</td><td>2011-05-31T00:00:00</td><td>4911-403C-98</td></tr><tr><td>43659</td><td>6</td><td>2</td><td>773</td><td>1</td><td>2039.994</td><td>0.0</td><td>4079.988</td><td>ce472532-a4c0-45ba-816e-eefd3fd848b3</td><td>2011-05-31T00:00:00</td><td>4911-403C-98</td></tr><tr><td>43659</td><td>7</td><td>1</td><td>774</td><td>1</td><td>2039.994</td><td>0.0</td><td>2039.994</td><td>80667840-f962-4ee3-96e0-aeca108e0d4f</td><td>2011-05-31T00:00:00</td><td>4911-403C-98</td></tr><tr><td>43659</td><td>8</td><td>3</td><td>714</td><td>1</td><td>28.8404</td><td>0.0</td><td>86.5212</td><td>e9d54907-e7b7-4969-80d9-76ba69f8a836</td><td>2011-05-31T00:00:00</td><td>4911-403C-98</td></tr><tr><td>43659</td><td>9</td><td>1</td><td>716</td><td>1</td><td>28.8404</td><td>0.0</td><td>28.8404</td><td>aa542630-bdcd-4ce5-89a0-c1bf82747725</td><td>2011-05-31T00:00:00</td><td>4911-403C-98</td></tr><tr><td>43659</td><td>10</td><td>6</td><td>709</td><td>1</td><td>5.7</td><td>0.0</td><td>34.2</td><td>ac769034-3c2f-495c-a5a7-3b71cdb25d4e</td><td>2011-05-31T00:00:00</td><td>4911-403C-98</td></tr><tr><td>43659</td><td>11</td><td>2</td><td>712</td><td>1</td><td>5.1865</td><td>0.0</td><td>10.373</td><td>06a66921-6b9f-4199-a912-ddafd383472b</td><td>2011-05-31T00:00:00</td><td>4911-403C-98</td></tr><tr><td>43659</td><td>12</td><td>4</td><td>711</td><td>1</td><td>20.1865</td><td>0.0</td><td>80.746</td><td>0e371ee3-253e-4bb0-b813-83cf4224f972</td><td>2011-05-31T00:00:00</td><td>4911-403C-98</td></tr><tr><td>43660</td><td>13</td><td>1</td><td>762</td><td>1</td><td>419.4589</td><td>0.0</td><td>419.4589</td><td>419a1302-ac7a-4044-97b2-66d9d14cd02e</td><td>2011-05-31T00:00:00</td><td>6431-4D57-83</td></tr><tr><td>43660</td><td>14</td><td>1</td><td>758</td><td>1</td><td>874.794</td><td>0.0</td><td>874.794</td><td>5d0b2b03-1d4c-4c34-9696-c14c58e7301c</td><td>2011-05-31T00:00:00</td><td>6431-4D57-83</td></tr><tr><td>43661</td><td>15</td><td>1</td><td>745</td><td>1</td><td>809.76</td><td>0.0</td><td>809.76</td><td>ede1759e-6733-4c7b-a43f-dc6f48002d8a</td><td>2011-05-31T00:00:00</td><td>4E0A-4F89-AE</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         43659,
         1,
         1,
         776,
         1,
         2024.994,
         0,
         2024.994,
         "b207c96d-d9e6-402b-8470-2cc176c42283",
         "2011-05-31T00:00:00",
         "4911-403C-98"
        ],
        [
         43659,
         2,
         3,
         777,
         1,
         2024.994,
         0,
         6074.982,
         "7abb600d-1e77-41be-9fe5-b9142cfc08fa",
         "2011-05-31T00:00:00",
         "4911-403C-98"
        ],
        [
         43659,
         3,
         1,
         778,
         1,
         2024.994,
         0,
         2024.994,
         "475cf8c6-49f6-486e-b0ad-afc6a50cdd2f",
         "2011-05-31T00:00:00",
         "4911-403C-98"
        ],
        [
         43659,
         4,
         1,
         771,
         1,
         2039.994,
         0,
         2039.994,
         "04c4de91-5815-45d6-8670-f462719fbce3",
         "2011-05-31T00:00:00",
         "4911-403C-98"
        ],
        [
         43659,
         5,
         1,
         772,
         1,
         2039.994,
         0,
         2039.994,
         "5a74c7d2-e641-438e-a7ac-37bf23280301",
         "2011-05-31T00:00:00",
         "4911-403C-98"
        ],
        [
         43659,
         6,
         2,
         773,
         1,
         2039.994,
         0,
         4079.988,
         "ce472532-a4c0-45ba-816e-eefd3fd848b3",
         "2011-05-31T00:00:00",
         "4911-403C-98"
        ],
        [
         43659,
         7,
         1,
         774,
         1,
         2039.994,
         0,
         2039.994,
         "80667840-f962-4ee3-96e0-aeca108e0d4f",
         "2011-05-31T00:00:00",
         "4911-403C-98"
        ],
        [
         43659,
         8,
         3,
         714,
         1,
         28.8404,
         0,
         86.5212,
         "e9d54907-e7b7-4969-80d9-76ba69f8a836",
         "2011-05-31T00:00:00",
         "4911-403C-98"
        ],
        [
         43659,
         9,
         1,
         716,
         1,
         28.8404,
         0,
         28.8404,
         "aa542630-bdcd-4ce5-89a0-c1bf82747725",
         "2011-05-31T00:00:00",
         "4911-403C-98"
        ],
        [
         43659,
         10,
         6,
         709,
         1,
         5.7,
         0,
         34.2,
         "ac769034-3c2f-495c-a5a7-3b71cdb25d4e",
         "2011-05-31T00:00:00",
         "4911-403C-98"
        ],
        [
         43659,
         11,
         2,
         712,
         1,
         5.1865,
         0,
         10.373,
         "06a66921-6b9f-4199-a912-ddafd383472b",
         "2011-05-31T00:00:00",
         "4911-403C-98"
        ],
        [
         43659,
         12,
         4,
         711,
         1,
         20.1865,
         0,
         80.746,
         "0e371ee3-253e-4bb0-b813-83cf4224f972",
         "2011-05-31T00:00:00",
         "4911-403C-98"
        ],
        [
         43660,
         13,
         1,
         762,
         1,
         419.4589,
         0,
         419.4589,
         "419a1302-ac7a-4044-97b2-66d9d14cd02e",
         "2011-05-31T00:00:00",
         "6431-4D57-83"
        ],
        [
         43660,
         14,
         1,
         758,
         1,
         874.794,
         0,
         874.794,
         "5d0b2b03-1d4c-4c34-9696-c14c58e7301c",
         "2011-05-31T00:00:00",
         "6431-4D57-83"
        ],
        [
         43661,
         15,
         1,
         745,
         1,
         809.76,
         0,
         809.76,
         "ede1759e-6733-4c7b-a43f-dc6f48002d8a",
         "2011-05-31T00:00:00",
         "4E0A-4F89-AE"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "SalesOrderID",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "SalesOrderDetailID",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "OrderQty",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "ProductID",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "SpecialOfferID",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "UnitPrice",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "UnitPriceDiscount",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "LineTotal",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "rowguid",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "ModifiedDate",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "CarrierTrackingNumber",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 38
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "SalesOrderID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "SalesOrderDetailID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "OrderQty",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ProductID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "SpecialOfferID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "UnitPrice",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "UnitPriceDiscount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "LineTotal",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "rowguid",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ModifiedDate",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CarrierTrackingNumber",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- 3. Verificar estrutura tabela (TESTE DE VALIDAÇÃO)\n",
    "-- Exemplo:\n",
    "SELECT * FROM ted_dev.dev_andre_silva.raw_api_sales_order_detail LIMIT 15;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2ea5b2d-be19-435b-ab91-7a08ecebf0ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_order_detail: 121,317 registros\n",
      "sales_order_header: 31,465 registros\n",
      "purchase_order_detail: 8,845 registros\n",
      "purchase_order_header: 4,012 registros\n"
     ]
    }
   ],
   "source": [
    "# 4. Verificar se o count() bate com o esperado (TESTE DE VALIDAÇÃO)\n",
    "# ================================X=================================\n",
    "for table in [\"sales_order_detail\", \"sales_order_header\", \"purchase_order_detail\", \"purchase_order_header\"]:\n",
    "    df = spark.table(f\"ted_dev.dev_andre_silva.raw_api_{table}\")\n",
    "    print(f\"{table}: {df.count():,} registros\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5563825141908134,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Extração das API",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
