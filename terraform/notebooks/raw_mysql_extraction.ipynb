{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e997c4e1-2f65-4a19-ac53-03673827659c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo: Sales.CreditCard\nTabela pequena (19118 linhas) — extraindo de uma vez.\nExtraindo: Sales.Customer\nTabela pequena (19820 linhas) — extraindo de uma vez.\nExtraindo: Sales.SalesOrderDetail\nTabela pequena (121317 linhas) — extraindo de uma vez.\nExtraindo: Sales.SalesOrderHeader\nTabela pequena (31465 linhas) — extraindo de uma vez.\nExtraindo: Sales.SalesOrderHeaderSalesReason\nTabela pequena (27647 linhas) — extraindo de uma vez.\nExtraindo: Sales.SalesPerson\nTabela pequena (17 linhas) — extraindo de uma vez.\nExtraindo: Sales.SalesReason\nTabela pequena (10 linhas) — extraindo de uma vez.\nExtraindo: Sales.SalesTerritory\nTabela pequena (10 linhas) — extraindo de uma vez.\nExtraindo: Person.Address\nTabela pequena (19614 linhas) — extraindo de uma vez.\nExtraindo: Person.BusinessEntity\nTabela pequena (20777 linhas) — extraindo de uma vez.\nExtraindo: Person.CountryRegion\nTabela pequena (238 linhas) — extraindo de uma vez.\nExtraindo: Person.Person\nTabela pequena (19972 linhas) — extraindo de uma vez.\nExtraindo: Person.StateProvince\nTabela pequena (181 linhas) — extraindo de uma vez.\nExtraindo: HumanResources.Employee\nTabela pequena (290 linhas) — extraindo de uma vez.\nExtraindo: Production.Product\nTabela pequena (578 linhas) — extraindo de uma vez.\nExtraindo: Production.ProductCategory\nTabela pequena (4 linhas) — extraindo de uma vez.\nExtraindo: Production.ProductSubcategory\nTabela pequena (37 linhas) — extraindo de uma vez.\nExtração concluída com sucesso.\n"
     ]
    }
   ],
   "source": [
    "## Importa as bibliotecas necessárias: SparkSession para trabalhar com Spark (extração dos dados) e DBUtils para trabalhar com variáveis sensíveis (variável de ambiente):\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RawExtraction\").getOrCreate()\n",
    "dbutils = DBUtils(spark) #Inicializa o utilitário DBUtils para acessar os secrets armazenados\n",
    "\n",
    "## Adicionando as credenciais do Secret Scope:\n",
    "jdbc_hostname = dbutils.secrets.get(scope=\"sqlserver_scope\", key=\"sql_host\")\n",
    "jdbc_port = dbutils.secrets.get(scope=\"sqlserver_scope\", key=\"sql_port\")\n",
    "jdbc_user = dbutils.secrets.get(scope=\"sqlserver_scope\", key=\"sql_user\")\n",
    "jdbc_password = dbutils.secrets.get(scope=\"sqlserver_scope\", key=\"sql_password\")\n",
    "\n",
    "## Recuperando de forma segura as credenciais de conexão do SQL Server que foram armazenadas no Secret Scope:\n",
    "jdbc_url = f\"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};databaseName=AdventureWorks;encrypt=false;trustServerCertificate=true\"\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": jdbc_user,\n",
    "    \"password\": jdbc_password,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "## Lista de tabelas a extrair com Formato (nome_tabela, schema_sql_server):\n",
    "tabelas_sql_server = [\n",
    "    (\"CreditCard\", \"Sales\"),\n",
    "    (\"Customer\", \"Sales\"),\n",
    "    (\"SalesOrderDetail\", \"Sales\"),\n",
    "    (\"SalesOrderHeader\", \"Sales\"),\n",
    "    (\"SalesOrderHeaderSalesReason\", \"Sales\"),\n",
    "    (\"SalesPerson\", \"Sales\"),\n",
    "    (\"SalesReason\", \"Sales\"),\n",
    "    (\"SalesTerritory\", \"Sales\"),\n",
    "    (\"Address\", \"Person\"),\n",
    "    (\"BusinessEntity\", \"Person\"),\n",
    "    (\"CountryRegion\", \"Person\"),\n",
    "    (\"Person\", \"Person\"),\n",
    "    (\"StateProvince\", \"Person\"),\n",
    "    (\"Employee\", \"HumanResources\"),\n",
    "    (\"Product\", \"Production\"),\n",
    "    (\"ProductCategory\", \"Production\"),\n",
    "    (\"ProductSubcategory\", \"Production\"),\n",
    "]\n",
    "\n",
    "## Parâmetros de paginação com tamanho de 1M:\n",
    "pagina_tamanho = 1_000_000\n",
    "\n",
    "## monta o nome das tabelas de origem e destino:\n",
    "for nome_tabela, schema in tabelas_sql_server:\n",
    "    tabela_fonte = f\"{schema}.{nome_tabela}\"\n",
    "    tabela_destino = f\"raw_{nome_tabela}\"\n",
    "    print(f\"Extraindo: {tabela_fonte}\")\n",
    "\n",
    "    ## Obtem número total de linhas:\n",
    "    count_query = f\"(SELECT COUNT(*) as total FROM {tabela_fonte}) AS count_table\"\n",
    "    total_rows = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=count_query,\n",
    "        properties=connection_properties\n",
    "    ).collect()[0][\"total\"]\n",
    "\n",
    "    if total_rows <= pagina_tamanho:\n",
    "        print(f\"Tabela pequena ({total_rows} linhas) — extraindo de uma vez.\")\n",
    "        df = spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=tabela_fonte,\n",
    "            properties=connection_properties\n",
    "        )\n",
    "        df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"ted_dev.dev_andre_silva.{tabela_destino}\")\n",
    "    else:\n",
    "        print(f\"Tabela grande ({total_rows} linhas) — extraindo em páginas.\")\n",
    "        num_pages = (total_rows // pagina_tamanho) + int(total_rows % pagina_tamanho != 0)\n",
    "\n",
    "        for i in range(num_pages):\n",
    "            offset = i * pagina_tamanho\n",
    "            print(f\"   ➤ Página {i+1}/{num_pages} (OFFSET {offset})\")\n",
    "            paged_query = f\"(SELECT * FROM {tabela_fonte} ORDER BY (SELECT NULL) OFFSET {offset} ROWS FETCH NEXT {pagina_tamanho} ROWS ONLY) AS paged_table\"\n",
    "\n",
    "            df_page = spark.read.jdbc(\n",
    "                url=jdbc_url,\n",
    "                table=paged_query,\n",
    "                properties=connection_properties\n",
    "            )\n",
    "\n",
    "            df_page.write.mode(\"append\").format(\"delta\").saveAsTable(f\"ted_dev.dev_andre_silva.{tabela_destino}\")\n",
    "\n",
    "print(\"Extração concluída com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17fc9e8b-85a4-4f45-b864-a27bf2e94f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Extração Mysql (BD)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}